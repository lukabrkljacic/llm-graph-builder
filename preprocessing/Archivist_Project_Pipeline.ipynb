{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7aKzN5PDTfE"
      },
      "source": [
        "# 0. Install if needed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_4TUKm2DDMbc",
        "outputId": "579a0d17-1afc-437f-b52a-14c3e30e4990"
      },
      "outputs": [],
      "source": [
        "%pip install \"markitdown[all]\" openai pdf2image pydantic Send2Trash PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Imports, basic definitions and option selections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUQ97W8rD13A",
        "outputId": "23c821c1-2350-44c3-80f4-f031137124f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\RAC62971\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
            "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
          ]
        }
      ],
      "source": [
        "# All needed imports\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import re\n",
        "import shutil\n",
        "import tempfile\n",
        "import logging\n",
        "import subprocess\n",
        "import time\n",
        "from pathlib import Path\n",
        "from send2trash import send2trash\n",
        "from openai import OpenAI\n",
        "from markitdown import MarkItDown\n",
        "from dotenv import load_dotenv\n",
        "from pdf2image import convert_from_path\n",
        "from PyPDF2 import PdfReader\n",
        "from typing import Dict, List, Optional, Union, Tuple, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set up the logging function\n",
        "def setup_logger(level=logging.INFO) -> logging.Logger:\n",
        "    logger = logging.getLogger(\"md_pipeline\")\n",
        "    if not logger.handlers:\n",
        "        handler = logging.StreamHandler(stream=sys.stdout)\n",
        "        formatter = logging.Formatter(\"[%(asctime)s] %(levelname)s %(message)s\", datefmt=\"%H:%M:%S\")\n",
        "        handler.setFormatter(formatter)\n",
        "        logger.addHandler(handler)\n",
        "    logger.setLevel(level)\n",
        "    logger.propagate = False\n",
        "    return logger\n",
        "\n",
        "LOGGER = setup_logger()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load in the API key from the top-level .env file\n",
        "load_dotenv(Path.cwd().parent / \".env\")\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Define the model we want to use to convert the PNGs into Markdown\n",
        "LLM_MODEL = \"gpt-5-mini\"\n",
        "\n",
        "LLM_CLIENT = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "# Read in the system prompts\n",
        "sys_prompt_detailed = (Path.cwd() / \"system_prompts\" / \"system_prompt_detailed.md\").read_text(encoding=\"utf-8\")\n",
        "sys_prompt_summarized = (Path.cwd() / \"system_prompts\" / \"system_prompt_summarized.md\").read_text(encoding=\"utf-8\")\n",
        "\n",
        "# Choose the system prompt you'd like to use. Detailed will attempt to keep as much of the original content\n",
        "# as possible, at the cost of speed and cost. Summarized will summarize much more of the content but it will\n",
        "# be fast and cost less to process the data\n",
        "sys_prompt_mode = \"detailed\" # can be 'detailed' or 'summarized'\n",
        "SYSTEM_PROMPT = sys_prompt_detailed if sys_prompt_mode == \"detailed\" else sys_prompt_summarized\n",
        "\n",
        "# Define input and output directories\n",
        "INPUT_BASE_DIR = Path(\"./input_data/not_processed\")\n",
        "OUTPUT_BASE_DIR = Path(\"./input_data/processed\")\n",
        "\n",
        "# Set the POPPLER_PATH environment variable\n",
        "os.environ[\"POPPLER_PATH\"] = r\"C:\\\\Users\\\\RAC62971\\\\Downloads\\\\poppler-25.07.0\\\\Library\\\\bin\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Temporary Conversions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PPT -> PDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Goes through a PowerPoint file and converts it to a temporary PDF. This PDF then goes through the regular PDF processing pipeline (see next section). The function tries, in order:\n",
        "\n",
        "1. Microsoft PowerPoint COM on Windows\n",
        "2. LibreOffice soffice\n",
        "3. unoconv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _have(cmd: str) -> bool:\n",
        "    return shutil.which(cmd) is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _ascii_safe_copy(src: Path) -> Path:\n",
        "    \"\"\"\n",
        "    Copy the PPTX to a short ASCII-only filename in a temp folder to avoid\n",
        "    COM issues with unicode, spaces, quotes, stars, or very long paths.\n",
        "    Returns the path to the copied file.\n",
        "    \"\"\"\n",
        "    tmp_root = Path(tempfile.mkdtemp(prefix=\"pptx2pdf_\"))\n",
        "    safe_name = re.sub(r\"[^A-Za-z0-9._-]+\", \"_\", src.stem)[:40] + src.suffix\n",
        "    safe_path = tmp_root / safe_name\n",
        "    shutil.copy2(src, safe_path)\n",
        "    return safe_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pptx_to_pdf(\n",
        "    pptx_path: Path,\n",
        "    out_dir: Path,\n",
        "    method: Optional[str] = None,\n",
        "    timeout: int = 600,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Convert PPTX to PDF and return the PDF path in out_dir.\n",
        "\n",
        "    Order tried:\n",
        "      1) PowerPoint COM on Windows using ExportAsFixedFormat, then SaveAs(32)\n",
        "      2) soffice\n",
        "      3) unoconv\n",
        "\n",
        "    Key stability choices:\n",
        "      - Copy to an ASCII-only short temp file before opening via COM\n",
        "      - Use DispatchEx + EnsureDispatch to isolate a clean instance\n",
        "      - Avoid touching Application.Visible\n",
        "      - Pass only supported params via keywords to ExportAsFixedFormat\n",
        "    \"\"\"\n",
        "    pptx_path = Path(pptx_path).resolve()\n",
        "    out_dir = Path(out_dir).resolve()\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    target_pdf = (out_dir / f\"{pptx_path.stem}.pdf\").resolve()\n",
        "\n",
        "    LOGGER.info(f\"Converting PPTX to PDF: {pptx_path.name}\")\n",
        "\n",
        "    # 1) PowerPoint COM path on Windows\n",
        "    if method == \"powerpoint\" or (method is None and os.name == \"nt\"):\n",
        "        try:\n",
        "            import platform\n",
        "            if platform.system() == \"Windows\":\n",
        "                # Copy to safe temp path to avoid unicode and long path issues\n",
        "                safe_src = _ascii_safe_copy(pptx_path)\n",
        "                LOGGER.info(f\"Copied to safe temp path for COM: {safe_src}\")\n",
        "\n",
        "                import win32com.client  # type: ignore\n",
        "                from win32com.client import gencache, constants  # type: ignore\n",
        "\n",
        "                LOGGER.info(\"Starting PowerPoint COM via DispatchEx\")\n",
        "                app = win32com.client.DispatchEx(\"PowerPoint.Application\")\n",
        "                # Avoid setting .Visible. It can throw.\n",
        "                # Reduce popups if possible\n",
        "                try:\n",
        "                    app.DisplayAlerts = 1  # ppAlertsNone\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "                pres = None\n",
        "                try:\n",
        "                    # Open with window suppressed but without toggling .Visible\n",
        "                    # Signature: Open(FileName, ReadOnly, Untitled, WithWindow)\n",
        "                    pres = app.Presentations.Open(\n",
        "                        FileName=str(safe_src),\n",
        "                        ReadOnly=1,\n",
        "                        Untitled=0,\n",
        "                        WithWindow=False\n",
        "                    )\n",
        "\n",
        "                    # Prefer ExportAsFixedFormat with minimal, supported kwargs\n",
        "                    exported = False\n",
        "                    try:\n",
        "                        # constants:\n",
        "                        #   ppFixedFormatTypePDF = 2\n",
        "                        #   ppFixedFormatIntentPrint = 2\n",
        "                        #   ppPrintAll = 1\n",
        "                        pres.ExportAsFixedFormat(\n",
        "                            Path=str(target_pdf),\n",
        "                            FixedFormatType=constants.ppFixedFormatTypePDF,\n",
        "                            Intent=constants.ppFixedFormatIntentPrint,\n",
        "                            FrameSlides=True,\n",
        "                            RangeType=constants.ppPrintAll,\n",
        "                            PrintHiddenSlides=False,\n",
        "                            IncludeDocProperties=True,\n",
        "                            KeepIRMSettings=True,\n",
        "                            DocStructureTags=True,\n",
        "                            BitmapMissingFonts=True,\n",
        "                            UseISO19005_1=False\n",
        "                        )\n",
        "                        exported = target_pdf.exists()\n",
        "                        if exported:\n",
        "                            LOGGER.info(f\"Wrote PDF via ExportAsFixedFormat: {target_pdf}\")\n",
        "                    except Exception as e:\n",
        "                        LOGGER.warning(f\"ExportAsFixedFormat failed, will try SaveAs(32): {e}\")\n",
        "\n",
        "                    if not exported:\n",
        "                        # Fallback: SaveAs with ppSaveAsPDF = 32\n",
        "                        pres.SaveAs(str(target_pdf), 32)\n",
        "                        exported = target_pdf.exists()\n",
        "                        if exported:\n",
        "                            LOGGER.info(f\"Wrote PDF via SaveAs: {target_pdf}\")\n",
        "\n",
        "                    if not exported:\n",
        "                        raise RuntimeError(\"PowerPoint did not produce the PDF file\")\n",
        "\n",
        "                    return target_pdf\n",
        "\n",
        "                finally:\n",
        "                    # Close presentation and quit app\n",
        "                    try:\n",
        "                        if pres is not None:\n",
        "                            pres.Close()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "                    try:\n",
        "                        app.Quit()\n",
        "                    except Exception:\n",
        "                        pass\n",
        "        except Exception as e:\n",
        "            LOGGER.warning(f\"PowerPoint COM path failed: {e}\")\n",
        "\n",
        "    # 2) LibreOffice soffice\n",
        "    if method == \"soffice\" or (method is None and _have(\"soffice\")):\n",
        "        try:\n",
        "            LOGGER.info(\"Using LibreOffice soffice\")\n",
        "            cmd = [\n",
        "                \"soffice\",\n",
        "                \"--headless\",\n",
        "                \"--invisible\",\n",
        "                \"--norestore\",\n",
        "                \"--convert-to\",\n",
        "                \"pdf\",\n",
        "                \"--outdir\",\n",
        "                str(out_dir),\n",
        "                str(pptx_path),\n",
        "            ]\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n",
        "            if target_pdf.exists():\n",
        "                LOGGER.info(f\"Wrote PDF via soffice: {target_pdf}\")\n",
        "                return target_pdf\n",
        "        except Exception as e:\n",
        "            LOGGER.warning(f\"soffice path failed: {e}\")\n",
        "\n",
        "    # 3) unoconv\n",
        "    if method == \"unoconv\" or (method is None and _have(\"unoconv\")):\n",
        "        try:\n",
        "            LOGGER.info(\"Using unoconv\")\n",
        "            cmd = [\n",
        "                \"unoconv\",\n",
        "                \"-f\", \"pdf\",\n",
        "                \"-o\", str(out_dir),\n",
        "                str(pptx_path),\n",
        "            ]\n",
        "            subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, timeout=timeout)\n",
        "            if target_pdf.exists():\n",
        "                LOGGER.info(f\"Wrote PDF via unoconv: {target_pdf}\")\n",
        "                return target_pdf\n",
        "        except Exception as e:\n",
        "            LOGGER.warning(f\"unoconv path failed: {e}\")\n",
        "\n",
        "    raise RuntimeError(\n",
        "        \"Could not convert PPTX to PDF. Install LibreOffice or unoconv, or ensure desktop PowerPoint is installed.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PDF -> PNG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-m4uQ17ERqT"
      },
      "source": [
        "Goes through a PDF and converts each page into an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def pdf_pages_to_pngs(\n",
        "    pdf_path: Union[str, Path],\n",
        "    dpi: int = 200,\n",
        "    poppler_path: Optional[str] = None,\n",
        ") -> Tuple[Path, List[Path]]:\n",
        "    \"\"\"\n",
        "    Convert every page of a PDF to PNGs stored in a temporary directory.\n",
        "    Returns (temp_dir, [png_paths]). Caller removes temp_dir.\n",
        "    \"\"\"\n",
        "    pdf_path = Path(pdf_path)\n",
        "    temp_dir = Path(tempfile.mkdtemp(prefix=f\"{pdf_path.stem}_pdfpages_\"))\n",
        "\n",
        "    # Page count estimate for better logs\n",
        "    try:\n",
        "        reader = PdfReader(str(pdf_path))\n",
        "        total_pages = len(reader.pages)\n",
        "    except Exception:\n",
        "        total_pages = None\n",
        "\n",
        "    if poppler_path is None:\n",
        "        poppler_path = os.environ.get(\"POPPLER_PATH\")\n",
        "\n",
        "    start = time.time()\n",
        "    if total_pages is not None:\n",
        "        LOGGER.info(f\"Rendering {total_pages} page(s) from {pdf_path.name} at {dpi} dpi via pdf2image\")\n",
        "    else:\n",
        "        LOGGER.info(f\"Rendering pages from {pdf_path.name} at {dpi} dpi via pdf2image\")\n",
        "\n",
        "    # Write directly to temp_dir so files appear incrementally on disk\n",
        "    png_paths = convert_from_path(\n",
        "        str(pdf_path),\n",
        "        dpi=dpi,\n",
        "        poppler_path=poppler_path,\n",
        "        output_folder=str(temp_dir),\n",
        "        paths_only=True,\n",
        "        fmt=\"png\",\n",
        "        output_file=f\"{pdf_path.stem}_page\"\n",
        "    )\n",
        "\n",
        "    elapsed = time.time() - start\n",
        "    LOGGER.info(f\"Rendering complete in {elapsed:.1f}s. Generated {len(png_paths)} image(s) in {temp_dir}\")\n",
        "    return temp_dir, [Path(p) for p in png_paths]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Markdown Conversions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _mirror_markdown_to_backend(md_path: Path) -> None:\n",
        "    \"\"\"Ensure backend/merged_files has a symlink to the generated markdown.\"\"\"\n",
        "    repo_root = None\n",
        "    for candidate in [Path.cwd()] + list(Path.cwd().parents):\n",
        "        backend_dir = candidate / \"backend\"\n",
        "        preprocessing_dir = candidate / \"preprocessing\"\n",
        "        if backend_dir.exists() and preprocessing_dir.exists():\n",
        "            repo_root = candidate\n",
        "            break\n",
        "    if repo_root is None:\n",
        "        LOGGER.warning(\"Skipping backend symlink; repository root not found.\")\n",
        "        return\n",
        "    merged_dir = repo_root / \"backend\" / \"merged_files\"\n",
        "    merged_dir.mkdir(parents=True, exist_ok=True)\n",
        "    link_path = merged_dir / md_path.name\n",
        "    try:\n",
        "        if link_path.exists() or link_path.is_symlink():\n",
        "            if link_path.is_dir() and not link_path.is_symlink():\n",
        "                LOGGER.warning(f\"Cannot replace directory at {link_path}; skipping symlink.\")\n",
        "                return\n",
        "            link_path.unlink()\n",
        "        link_path.symlink_to(md_path.resolve())\n",
        "        LOGGER.info(f\"Symlinked {md_path} to {link_path}\")\n",
        "    except OSError as err:\n",
        "        LOGGER.warning(f\"Failed to symlink {md_path} into backend/merged_files: {err}\")\n",
        "        \n",
        "def convert_to_markdown_file(\n",
        "    input_value: Union[str, Path],\n",
        "    kind: str,\n",
        "    use_llm: bool = False,\n",
        "    llm_client=None,\n",
        "    llm_model: Optional[str] = None,\n",
        "    llm_prompt: Optional[str] = None,\n",
        "    out_dir: Optional[Union[str, Path]] = None,\n",
        "    out_name: Optional[str] = None,\n",
        "    project_context: Optional[Dict] = None\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Convert an input file or HTML string to Markdown using MarkItDown \n",
        "    and write it to ./processed/{project_name}/markdown/{file_name}.md\n",
        "\n",
        "    Parameters\n",
        "    - input_value\n",
        "      - For file-based kinds, pass a filesystem path.\n",
        "      - For 'html_string', pass a raw HTML string.\n",
        "    - kind: one of {'docx','xlsx','xls','html','htm','txt','log','csv','html_string', \"png\", \"pptx\", \"pdf\"}\n",
        "    - use_llm: default False. If True, provide llm_client and llm_model.\n",
        "    - llm_client, llm_model, llm_prompt: forwarded to MarkItDown when use_llm=True\n",
        "    - out_dir: optional override of the output directory. Defaults to ./input_data/processed\n",
        "    - out_name: optional base filename without extension. Defaults to input file stem when applicable.\n",
        "    - project_context: dict containing project metadata that gets added to markdown output\n",
        "\n",
        "    Returns\n",
        "    - Path to the written Markdown file.\n",
        "    \"\"\"\n",
        "    # 1) Normalize kind and choose default output directory bucket\n",
        "    file_type = kind.lower().strip()\n",
        "    allowed = {\"docx\", \"xlsx\", \"xls\", \"html\", \"htm\", \"txt\", \"log\", \"csv\", \"html_string\", \"png\", \"pptx\", \"pdf\"}\n",
        "    if file_type not in allowed:\n",
        "        raise ValueError(f\"kind must be one of {allowed}\")\n",
        "\n",
        "    if out_dir is None:\n",
        "        out_dir = \"./input_data/processed\"\n",
        "    out_dir = Path(out_dir)\n",
        "    out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 2) Add project context to the system prompt\n",
        "    if project_context:\n",
        "        context_str = \"#### PROJECT CONTEXT\\n\"\n",
        "        for key, value in project_context.items():\n",
        "            context_str += f\"- **{key}:** {value}\\n\"\n",
        "        context_str += \"\\n\"\n",
        "        llm_prompt = context_str + (llm_prompt or \"\")\n",
        "\n",
        "    # 3) Build MarkItDown converter\n",
        "    from markitdown import MarkItDown\n",
        "    if use_llm:\n",
        "        if llm_client is None or llm_model is None:\n",
        "            raise ValueError(\"use_llm=True requires llm_client and llm_model\")\n",
        "        converter = MarkItDown(llm_client=llm_client, llm_model=llm_model, llm_prompt=llm_prompt)\n",
        "    else:\n",
        "        converter = MarkItDown()\n",
        "\n",
        "    # 4) Convert depending on kind\n",
        "    if file_type == \"html_string\":\n",
        "        if not out_name:\n",
        "            raise ValueError(\"out_name is required when kind='html_string'\")\n",
        "        res = converter.convert_html(str(input_value))\n",
        "        base_name = out_name\n",
        "    else:\n",
        "        p = Path(input_value)\n",
        "        res = converter.convert(str(p))\n",
        "        base_name = out_name or p.stem\n",
        "\n",
        "    # 5) Write markdown\n",
        "    out_path = out_dir / f\"{base_name}.md\"\n",
        "    out_path.write_text(res.text_content, encoding=\"utf-8\")\n",
        "    print(f'Wrote {base_name}.md to {out_path}')\n",
        "    _mirror_markdown_to_backend(out_path)\n",
        "    return out_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Converts a list of images into a single Markdown file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def images_to_single_markdown(\n",
        "    image_paths: List[Path],\n",
        "    out_md_path: Path,\n",
        "    project_context: Optional[dict],\n",
        "    llm_client,\n",
        "    llm_model: str,\n",
        "    llm_prompt: str,\n",
        "    per_page_heading: bool = True,\n",
        "    log_every: int = 1,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Convert a list of images to one markdown file via MarkItDown.\n",
        "    Logs progress per page. Writes once to out_md_path.\n",
        "    \"\"\"\n",
        "    md_converter = MarkItDown(\n",
        "        llm_client=llm_client,\n",
        "        llm_model=llm_model,\n",
        "        llm_prompt=llm_prompt,\n",
        "    )\n",
        "\n",
        "    total = len(image_paths)\n",
        "    parts: List[str] = []\n",
        "\n",
        "    if project_context:\n",
        "        context_lines = [f\"- **{k}**: {v}\" for k, v in project_context.items()]\n",
        "        parts.append(\"### Project context\\n\" + \"\\n\".join(context_lines) + \"\\n\")\n",
        "\n",
        "    LOGGER.info(f\"Starting LLM extraction for {total} page image(s)\")\n",
        "    start = time.time()\n",
        "\n",
        "    for idx, img_path in enumerate(image_paths, start=1):\n",
        "        t0 = time.time()\n",
        "        try:\n",
        "            res = md_converter.convert(str(img_path))\n",
        "            page_md = res.text_content if hasattr(res, \"text_content\") else str(res)\n",
        "            ok = True\n",
        "        except Exception as e:\n",
        "            page_md = f\"_Extraction failed for {img_path.name}: {e}_\"\n",
        "            ok = False\n",
        "\n",
        "        if per_page_heading:\n",
        "            parts.append(f\"\\n## Page {idx}\\n\")\n",
        "        parts.append(page_md.strip())\n",
        "\n",
        "        if log_every and (idx % log_every == 0 or idx == total):\n",
        "            status = \"ok\" if ok else \"error\"\n",
        "            pct = int((idx / total) * 100) if total else 100\n",
        "            LOGGER.info(f\"Extracted page {idx}/{total} [{pct}%] in {time.time() - t0:.1f}s ({status})\")\n",
        "\n",
        "    out_md_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_md_path.write_text(\"\\n\\n\".join(parts), encoding=\"utf-8\")\n",
        "\n",
        "    LOGGER.info(f\"Wrote combined markdown to {out_md_path} in {time.time() - start:.1f}s\")\n",
        "    _mirror_markdown_to_backend(out_md_path)\n",
        "    return out_md_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Process a Project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _unique_destination(dest_dir: Path, name: str) -> Path:\n",
        "    \"\"\"Return a unique path inside dest_dir for name without overwriting existing files.\"\"\"\n",
        "    candidate = dest_dir / name\n",
        "    if not candidate.exists():\n",
        "        return candidate\n",
        "    stem, suffix = Path(name).stem, Path(name).suffix\n",
        "    i = 1\n",
        "    while True:\n",
        "        nxt = dest_dir / f\"{stem}__copy_{i}{suffix}\"\n",
        "        if not nxt.exists():\n",
        "            return nxt\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_project(project_name: str):\n",
        "    \"\"\"\n",
        "    Process an entire project directory.\n",
        "    \n",
        "    Args:\n",
        "        project_name (str): Name of the project directory\n",
        "    \"\"\"\n",
        "    print(f\"Processing project: {project_name}\")\n",
        "\n",
        "    # Get input and output paths\n",
        "    project_input_dir = INPUT_BASE_DIR / project_name\n",
        "    project_output_dir = OUTPUT_BASE_DIR / project_name\n",
        "\n",
        "    # Create output directory structure\n",
        "    markdown_dir = project_output_dir / \"markdown\"\n",
        "    originals_dir = project_output_dir / \"original_files\"\n",
        "    markdown_dir.mkdir(parents=True, exist_ok=True)\n",
        "    originals_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Read project metadata\n",
        "    metadata_path = project_input_dir / \"metadata.json\"\n",
        "    project_metadata = {}\n",
        "    if metadata_path.exists():\n",
        "        with open(metadata_path, 'r') as f:\n",
        "            project_metadata = json.load(f)\n",
        "\n",
        "    for file_path in project_input_dir.iterdir():\n",
        "        if file_path.is_file() and file_path.name != \"metadata.json\":\n",
        "            try:\n",
        "                \n",
        "                # Determine file type\n",
        "                file_ext = file_path.suffix.lower()[1:]  # Remove the dot\n",
        "\n",
        "                if file_ext in [\"pptx\"]:\n",
        "                    LOGGER.info(f\"PPTX pipeline started for {file_path.name}\")\n",
        "\n",
        "                    # 0) Create a temp folder for the transient PDF\n",
        "                    tmp_pdf_root = Path(tempfile.mkdtemp(prefix=f\"{file_path.stem}_tmp_pdf_\"))\n",
        "\n",
        "                    try:\n",
        "                        # 1) Convert PPTX to a temporary PDF\n",
        "                        pdf_path = pptx_to_pdf(\n",
        "                            pptx_path=file_path,\n",
        "                            out_dir=tmp_pdf_root,\n",
        "                            method=None  # or \"powerpoint\" | \"soffice\" | \"unoconv\"\n",
        "                        )\n",
        "\n",
        "                        # 2) Render the PDF to images in a temp dir\n",
        "                        tmp_img_dir, png_paths = pdf_pages_to_pngs(\n",
        "                            pdf_path=pdf_path,\n",
        "                            dpi=200,\n",
        "                            poppler_path=os.environ.get(\"POPPLER_PATH\")\n",
        "                        )\n",
        "\n",
        "                        # 3) Convert images to a single combined markdown\n",
        "                        combined_out_md = markdown_dir / f\"{file_path.stem}.md\"\n",
        "                        images_to_single_markdown(\n",
        "                            image_paths=sorted(png_paths, key=lambda p: p.name),\n",
        "                            out_md_path=combined_out_md,\n",
        "                            project_context=project_metadata,\n",
        "                            llm_client=LLM_CLIENT,\n",
        "                            llm_model=LLM_MODEL,\n",
        "                            llm_prompt=SYSTEM_PROMPT,\n",
        "                            per_page_heading=True,\n",
        "                            log_every=1\n",
        "                        )\n",
        "\n",
        "                        # 4) Clean up temporary images\n",
        "                        try:\n",
        "                            shutil.rmtree(tmp_img_dir, ignore_errors=True)\n",
        "                            LOGGER.info(f\"Removed temporary image directory {tmp_img_dir}\")\n",
        "                        except Exception as cleanup_err:\n",
        "                            LOGGER.warning(f\"Failed to remove temp image folder {tmp_img_dir}: {cleanup_err}\")\n",
        "\n",
        "                        # 5) We do not keep the transient PDF\n",
        "                        try:\n",
        "                            shutil.rmtree(tmp_pdf_root, ignore_errors=True)\n",
        "                            LOGGER.info(f\"Removed temporary PDF directory {tmp_pdf_root}\")\n",
        "                        except Exception as cleanup_err:\n",
        "                            LOGGER.warning(f\"Failed to remove temp PDF folder {tmp_pdf_root}: {cleanup_err}\")\n",
        "\n",
        "                        output_path = combined_out_md\n",
        "                        LOGGER.info(f\"PPTX pipeline complete for {file_path.name}\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        # Attempt to clean temp dir on error as well\n",
        "                        try:\n",
        "                            shutil.rmtree(tmp_pdf_root, ignore_errors=True)\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                        raise\n",
        "\n",
        "                elif file_ext in [\"pdf\"]:\n",
        "                    LOGGER.info(f\"PDF pipeline start for {file_path.name}\")\n",
        "                    \n",
        "                    # 1) Render all pages to PNGs in a temp dir\n",
        "                    tmp_dir, png_paths = pdf_pages_to_pngs(\n",
        "                        pdf_path=file_path,\n",
        "                        dpi=200,\n",
        "                        poppler_path=os.environ.get(\"POPPLER_PATH\")\n",
        "                    )\n",
        "\n",
        "                    # 2) Convert all images to one combined markdown\n",
        "                    combined_out_md = markdown_dir / f\"{file_path.stem}.md\"\n",
        "                    images_to_single_markdown(\n",
        "                        image_paths=sorted(png_paths, key=lambda p: p.name),\n",
        "                        out_md_path=combined_out_md,\n",
        "                        project_context=project_metadata,\n",
        "                        llm_client=LLM_CLIENT,\n",
        "                        llm_model=LLM_MODEL,\n",
        "                        llm_prompt=SYSTEM_PROMPT,\n",
        "                        per_page_heading=True,\n",
        "                        log_every=1        # increase to 2 or 5 if you want fewer log lines\n",
        "                    )\n",
        "\n",
        "                    # 3) Clean up temporary images\n",
        "                    try:\n",
        "                        shutil.rmtree(tmp_dir, ignore_errors=True)\n",
        "                        LOGGER.info(f\"Removed temporary image directory {tmp_dir}\")\n",
        "                    except Exception as cleanup_err:\n",
        "                        LOGGER.warning(f\"Failed to remove temp image folder {tmp_dir}: {cleanup_err}\")\n",
        "\n",
        "                    output_path = combined_out_md\n",
        "                    LOGGER.info(f\"PDF pipeline complete for {file_path.name}\")\n",
        "\n",
        "                elif file_ext in [\"png\"]:\n",
        "                #     # Convert the images to markdown via the image_to_markdown() function ()\n",
        "                #     print(\"images processed\")\n",
        "                    output_path = convert_to_markdown_file(\n",
        "                        input_value=str(file_path),\n",
        "                        kind=file_ext,\n",
        "                        use_llm=True,\n",
        "                        llm_client=LLM_CLIENT,\n",
        "                        llm_model=LLM_MODEL,\n",
        "                        llm_prompt=SYSTEM_PROMPT,\n",
        "                        out_dir=markdown_dir,\n",
        "                        project_context=project_metadata\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    # Process other formats normally\n",
        "                    output_path = convert_to_markdown_file(\n",
        "                        input_value=str(file_path),\n",
        "                        kind=file_ext,\n",
        "                        use_llm=False,\n",
        "                        out_dir=markdown_dir\n",
        "                )\n",
        "                    \n",
        "                # Copy the original into /original_files before deletion\n",
        "                dest = _unique_destination(originals_dir, file_path.name)\n",
        "                shutil.copy2(file_path, dest)\n",
        "\n",
        "                # Now remove the source from the input folder by sending to recycle bin\n",
        "                send2trash(str(file_path))\n",
        "            \n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {file_path.name}: {e}\")\n",
        "                continue\n",
        "    \n",
        "    print(f\"Completed processing project: {project_name}\")\n",
        "\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing project: moadchat\n",
            "[11:16:00] INFO PDF pipeline start for Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.pdf\n",
            "[11:16:00] INFO Rendering 12 page(s) from Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.pdf at 200 dpi via pdf2image\n",
            "[11:16:09] INFO Rendering complete in 9.5s. Generated 12 image(s) in C:\\Users\\RAC62971\\AppData\\Local\\Temp\\Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1_pdfpages_tiz5479b\n",
            "[11:16:09] INFO Starting LLM extraction for 12 page image(s)\n",
            "[11:16:39] INFO Extracted page 1/12 [8%] in 30.0s (ok)\n",
            "[11:17:10] INFO Extracted page 2/12 [16%] in 31.0s (ok)\n",
            "[11:17:45] INFO Extracted page 3/12 [25%] in 34.3s (ok)\n",
            "[11:18:14] INFO Extracted page 4/12 [33%] in 29.5s (ok)\n",
            "[11:18:47] INFO Extracted page 5/12 [41%] in 33.1s (ok)\n",
            "[11:19:25] INFO Extracted page 6/12 [50%] in 37.8s (ok)\n",
            "[11:20:00] INFO Extracted page 7/12 [58%] in 35.3s (ok)\n",
            "[11:20:28] INFO Extracted page 8/12 [66%] in 27.4s (ok)\n",
            "[11:20:58] INFO Extracted page 9/12 [75%] in 30.5s (ok)\n",
            "[11:21:34] INFO Extracted page 10/12 [83%] in 35.7s (ok)\n",
            "[11:22:15] INFO Extracted page 11/12 [91%] in 41.2s (ok)\n",
            "[11:24:16] INFO Extracted page 12/12 [100%] in 121.3s (ok)\n",
            "[11:24:16] INFO Wrote combined markdown to input_data\\processed\\moadchat\\markdown\\Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md in 487.1s\n",
            "[11:24:16] WARNING Failed to symlink input_data\\processed\\moadchat\\markdown\\Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md into backend/merged_files: [WinError 1314] A required privilege is not held by the client: 'C:\\\\Users\\\\RAC62971\\\\Documents\\\\Work\\\\llm-graph-builder\\\\preprocessing\\\\input_data\\\\processed\\\\moadchat\\\\markdown\\\\Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md' -> 'c:\\\\Users\\\\RAC62971\\\\Documents\\\\Work\\\\llm-graph-builder\\\\backend\\\\merged_files\\\\Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md'\n",
            "[11:24:16] INFO Removed temporary image directory C:\\Users\\RAC62971\\AppData\\Local\\Temp\\Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1_pdfpages_tiz5479b\n",
            "[11:24:16] INFO PDF pipeline complete for Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.pdf\n",
            "Completed processing project: moadchat\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "WindowsPath('input_data/processed/moadchat/markdown/Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md')"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "process_project(\"moadchat\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
