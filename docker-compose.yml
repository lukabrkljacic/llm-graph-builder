services:
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    volumes:
      - ./backend:/code
      - hf_cache:/root/.cache/huggingface
      - torch_cache:/root/.cache/torch
    env_file:
      - ./.env
    depends_on:
      - neo4j
    environment:
      - NEO4J_URI=${NEO4J_URI-bolt://neo4j:7687}
      - NEO4J_PASSWORD=${NEO4J_PASSWORD-letmein123}
      - NEO4J_USERNAME=${NEO4J_USERNAME-neo4j}
      - NEO4J_DATABASE=${NEO4J_DATABASE-neo4j}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - OPENAI_EMBEDDING_MODEL=${OPENAI_EMBEDDING_MODEL-text-embedding-3-small}
      - LANGCHAIN_ENDPOINT=${LANGCHAIN_ENDPOINT-}
      - LANGCHAIN_TRACING_V2=${LANGCHAIN_TRACING_V2-}
      - LANGCHAIN_PROJECT=${LANGCHAIN_PROJECT-}
      - LANGCHAIN_API_KEY=${LANGCHAIN_API_KEY-}
      - LANGCHAIN_OPENAI_API_KEY=${OPENAI_API_KEY}
      - KNN_MIN_SCORE=${KNN_MIN_SCORE-0.94}
      - IS_EMBEDDING=${IS_EMBEDDING-true}
      - GEMINI_ENABLED=${GEMINI_ENABLED-False}
      - GCP_LOG_METRICS_ENABLED=${GCP_LOG_METRICS_ENABLED-False}
      - UPDATE_GRAPH_CHUNKS_PROCESSED=${UPDATE_GRAPH_CHUNKS_PROCESSED-20}
      - NUMBER_OF_CHUNKS_TO_COMBINE=${NUMBER_OF_CHUNKS_TO_COMBINE-6}
      - ENTITY_EMBEDDING=${ENTITY_EMBEDDING-False}
      - GCS_FILE_CACHE=${GCS_FILE_CACHE-False}
#      - LLM_MODEL_CONFIG_openai_gpt_4o='{"provider":"openai","model":"gpt-4o-2024-11-20","api_key":"${OPENAI_API_KEY}","base_url":"https://api.openai.com/v1"}'
      - LLM_MODEL_CONFIG_openai_gpt_4o_mini='{"provider":"openai","model":"gpt-4o-mini-2024-07-18","api_key":"${OPENAI_API_KEY}","base_url":"https://api.openai.com/v1"}'
      - LLM_MODEL_CONFIG_openai_gpt_5_mini='{"provider":"openai","model":"gpt-5-mini","api_key":"${OPENAI_API_KEY}","base_url":"https://api.openai.com/v1"}'
#      - LLM_MODEL_CONFIG_anthropic_claude_35_sonnet=${LLM_MODEL_CONFIG_anthropic_claude_35_sonnet-}
#      - LLM_MODEL_CONFIG_fireworks_llama_v3_70b=${LLM_MODEL_CONFIG_fireworks_llama_v3_70b-}
#      - LLM_MODEL_CONFIG_azure_ai_gpt_4o=${LLM_MODEL_CONFIG_azure_ai_gpt_4o-}
#      - LLM_MODEL_CONFIG_azure_ai_gpt_35=${LLM_MODEL_CONFIG_azure_ai_gpt_35-}
#      - LLM_MODEL_CONFIG_groq_llama3_70b=${LLM_MODEL_CONFIG_groq_llama3_70b-}
#      - LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet=${LLM_MODEL_CONFIG_bedrock_claude_3_5_sonnet-}
#     - LLM_MODEL_CONFIG_fireworks_qwen_72b=${LLM_MODEL_CONFIG_fireworks_qwen_72b-}
#      - LLM_MODEL_CONFIG_ollama_llama3=${LLM_MODEL_CONFIG_ollama_llama3-}
      - GUNICORN_CMD_ARGS=--workers=1 --threads=4 --timeout=120
    # container_name: backend
    # extra_hosts:
    #   - host.docker.internal:host-gateway
    ports:
      - "8000:8000"
    networks:
      - net

  frontend:
    depends_on:
      - backend
    build:
      context: ./frontend
      dockerfile: Dockerfile
      args:
        - VITE_BACKEND_API_URL=${VITE_BACKEND_API_URL-http://localhost:8000}
        - VITE_REACT_APP_SOURCES=${VITE_REACT_APP_SOURCES-local,wiki,s3}
        - VITE_GOOGLE_CLIENT_ID=${VITE_GOOGLE_CLIENT_ID-}
        - VITE_BLOOM_URL=${VITE_BLOOM_URL-https://workspace-preview.neo4j.io/workspace/explore?connectURL={CONNECT_URL}&search=Show+me+a+graph&featureGenAISuggestions=true&featureGenAISuggestionsInternal=true}
        - VITE_TIME_PER_PAGE=${VITE_TIME_PER_PAGE-50}
        - VITE_CHUNK_SIZE=${VITE_CHUNK_SIZE-5242880}
        - VITE_LARGE_FILE_SIZE=${VITE_LARGE_FILE_SIZE-5242880}
        - VITE_ENV=${VITE_ENV-DEV}
        - VITE_CHAT_MODES=${VITE_CHAT_MODES-}
        - VITE_BATCH_SIZE=${VITE_BATCH_SIZE-2}
        - VITE_LLM_MODELS=${VITE_LLM_MODELS-openai_gpt_5_mini}
        - VITE_LLM_MODELS_PROD=${VITE_LLM_MODELS_PROD-openai_gpt_5_mini}
        - VITE_AUTH0_DOMAIN=${VITE_AUTH0_DOMAIN-}
        - VITE_AUTH0_CLIENT_ID=${VITE_AUTH0_CLIENT_ID-}
        - VITE_SKIP_AUTH=${VITE_SKIP_AUTH-true}
        - VITE_CHUNK_OVERLAP=${VITE_CHUNK_OVERLAP-}
        - VITE_TOKENS_PER_CHUNK=${VITE_TOKENS_PER_CHUNK-}
        - VITE_CHUNK_TO_COMBINE=${VITE_CHUNK_TO_COMBINE-}
        - DEPLOYMENT_ENV=local
    volumes:
      - ./frontend:/app
      - /app/node_modules
    env_file:
      - ./frontend/.env
    # container_name: frontend
    ports:
      - "8080:8080"
    networks:
      - net

  neo4j:
    image: neo4j:5
    container_name: neo4j
    environment:
      NEO4J_AUTH: neo4j/letmein123
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_dbms_security_auth__enabled: "true"
      NEO4J_server_memory_heap_initial__size: 1G
      NEO4J_server_memory_heap_max__size: 1G
    ports:
      - "7474:7474"
      - "7687:7687"
    networks:
      - net
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs

volumes:
  neo4j_data:
  neo4j_logs:
  hf_cache:
  torch_cache:

networks:
  net: