{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b112f9e",
   "metadata": {},
   "source": [
    "# Programmatic ingestion via the REST API\n",
    "\n",
    "The frontend normally drives the ingestion workflow by calling three FastAPI endpoints in sequence:\n",
    "\n",
    "1. [`/upload`](../backend/score.py) registers a `Document` node and stores the original file under `backend/merged_files`.\n",
    "2. [`/extract`](../backend/score.py) chunks the text, generates embeddings, writes graph triples, and updates the document status.\n",
    "3. [`/post_processing`](../backend/score.py) materialises optional indices (vector + hybrid search, similarity graph, etc.).\n",
    "\n",
    "When you already have a project that has been preprocessed into Markdown files the UI can be bypassed entirely. Run the backend as normal (for example with `uvicorn score:app --reload` from the `backend` directory or through Docker) and then call the helper located in `backend/scripts/bulk_ingest_via_api.py`.\n",
    "\n",
    "### Use directly from Python (ideal for notebooks)\n",
    "\n",
    "The helper exposes an `ingest_project_via_api` function so you can trigger ingestion from Python without touching the command line. This works well at the end of your preprocessing notebook:\n",
    "\n",
    "```python\n",
    "from backend.scripts.bulk_ingest_via_api import ingest_project_via_api\n",
    "\n",
    "ingest_project_via_api(\n",
    "    project_name=\"PROJECT_NAME\",\n",
    "    base_url=\"http://localhost:8000\",\n",
    "    uri=\"neo4j://neo4j:7687\",\n",
    "    user=\"neo4j\",\n",
    "    password=\"letmein123\",\n",
    "    database=\"neo4j\",\n",
    ")\n",
    "```\n",
    "\n",
    "All parameters fall back to the same environment variables that the UI relies on (`LLM_GRAPH_BUILDER_BASE_URL`, `NEO4J_URI`, etc.), so you can omit explicit arguments when they are already configured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d88f0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path.cwd().resolve()\n",
    "while repo_root != repo_root.parent and not (repo_root / \"backend\").exists():\n",
    "    repo_root = repo_root.parent\n",
    "\n",
    "sys.path.insert(0, str(repo_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b30deb4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO - Uploading Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md\n",
      "INFO - Extracting Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md\n"
     ]
    },
    {
     "ename": "IngestionError",
     "evalue": "Endpoint http://localhost:8000/extract returned non-success status: {'status': 'Failed', 'error': \"'>' not supported between instances of 'NoneType' and 'NoneType'\", 'message': \"Failed To Process File:Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md or LLM Unable To Parse Content '>' not supported between instances of 'NoneType' and 'NoneType'\", 'file_name': 'Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIngestionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mscripts\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbulk_ingest_via_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ingest_project_via_api\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mingest_project_via_api\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmoadchat\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhttp://localhost:8000\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43muri\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneo4j://neo4j:7687\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneo4j\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mletmein123\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdatabase\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mneo4j\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Work\\llm-graph-builder\\backend\\scripts\\bulk_ingest_via_api.py:354\u001b[39m, in \u001b[36mingest_project_via_api\u001b[39m\u001b[34m(project_name, processed_root, base_url, model, uri, user, password, database, token_chunk_size, chunk_overlap, chunks_to_combine, retry_condition, additional_instructions, post_processing_tasks, skip_post_processing, env_file, log_level, session)\u001b[39m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m markdown_files:\n\u001b[32m    353\u001b[39m     upload_markdown(session, base_url, file_path, auth_payload, model)\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m     \u001b[43mextract_markdown\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m        \u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mauth_payload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_chunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_overlap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunks_to_combine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks_to_combine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_condition\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_condition\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    364\u001b[39m \u001b[43m        \u001b[49m\u001b[43madditional_instructions\u001b[49m\u001b[43m=\u001b[49m\u001b[43madditional_instructions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_post_processing \u001b[38;5;129;01mand\u001b[39;00m post_processing_tasks:\n\u001b[32m    368\u001b[39m     run_post_processing(session, base_url, auth_payload, post_processing_tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Work\\llm-graph-builder\\backend\\scripts\\bulk_ingest_via_api.py:145\u001b[39m, in \u001b[36mextract_markdown\u001b[39m\u001b[34m(session, base_url, file_name, auth_payload, model, token_chunk_size, chunk_overlap, chunks_to_combine, retry_condition, additional_instructions)\u001b[39m\n\u001b[32m    133\u001b[39m url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_url.rstrip(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mEXTRACT_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    134\u001b[39m data = _create_form_payload(\n\u001b[32m    135\u001b[39m     auth_payload,\n\u001b[32m    136\u001b[39m     model=model,\n\u001b[32m   (...)\u001b[39m\u001b[32m    143\u001b[39m     additional_instructions=additional_instructions,\n\u001b[32m    144\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_post_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Documents\\Work\\llm-graph-builder\\backend\\scripts\\bulk_ingest_via_api.py:69\u001b[39m, in \u001b[36m_post_json\u001b[39m\u001b[34m(session, url, data, files, timeout)\u001b[39m\n\u001b[32m     67\u001b[39m status = payload.get(\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status != \u001b[33m\"\u001b[39m\u001b[33mSuccess\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IngestionError(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEndpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m returned non-success status: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpayload\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m payload\n",
      "\u001b[31mIngestionError\u001b[39m: Endpoint http://localhost:8000/extract returned non-success status: {'status': 'Failed', 'error': \"'>' not supported between instances of 'NoneType' and 'NoneType'\", 'message': \"Failed To Process File:Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md or LLM Unable To Parse Content '>' not supported between instances of 'NoneType' and 'NoneType'\", 'file_name': 'Componentization- Decomposing Monolithic LLM Responses into Manipulable Semantic Units V1.md'}"
     ]
    }
   ],
   "source": [
    "from backend.scripts.bulk_ingest_via_api import ingest_project_via_api\n",
    "\n",
    "ingest_project_via_api(\n",
    "    project_name=\"moadchat\",\n",
    "    base_url=\"http://localhost:8000\",\n",
    "    uri=\"neo4j://neo4j:7687\",\n",
    "    user=\"neo4j\",\n",
    "    password=\"letmein123\",\n",
    "    database=\"neo4j\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800ce1fd",
   "metadata": {},
   "source": [
    "### Run as a CLI (optional)\n",
    "\n",
    "If you prefer to drive things from a shell the module still ships a CLI:\n",
    "\n",
    "```bash\n",
    "python backend/scripts/bulk_ingest_via_api.py PROJECT_NAME \\\n",
    "  --base-url http://localhost:8000 \\\n",
    "  --uri neo4j://neo4j:7687 \\\n",
    "  --user neo4j \\\n",
    "  --password letmein123 \\\n",
    "  --database neo4j\n",
    "```\n",
    "\n",
    "The helper assumes the Markdown files live under `preprocessing/input_data/processed/PROJECT_NAME/markdown`. Each file is uploaded as a single chunk, extracted, and (optionally) post-processed. The defaults can be overridden with CLI flags:\n",
    "\n",
    "| Flag | Purpose |\n",
    "|------|---------|\n",
    "| `--model` | Embedding model stored on the `Document` node (defaults to `EMBEDDING_MODEL` or `all-MiniLM-L6-v2`). |\n",
    "| `--token-chunk-size`, `--chunk-overlap`, `--chunks-to-combine` | Forwarded directly to the `/extract` endpoint for chunking control. |\n",
    "| `--retry-condition`, `--additional-instructions` | Passed through unchanged to reuse UI features. |\n",
    "| `--post-processing-tasks` | Space-separated list of tasks for `/post_processing` (defaults to enabling hybrid search and chunk similarities). |\n",
    "| `--skip-post-processing` | Disable the `/post_processing` step entirely. |\n",
    "| `--env-file` | Load backend credentials from a `.env` file so the command stays short. |\n",
    "\n",
    "All responses are checked for a `status` of `\"Success\"`; a non-success result or HTTP error raises a descriptive exception so failures become visible in CI and automation pipelines. The script also logs progress, making it easy to follow along when large batches are processed.\n",
    "\n",
    "Because the backend still enforces that local files originate in `backend/merged_files`, the upload step continues to use the standard `/upload` endpoint. No extra symlinks or directory changes are necessaryâ€”the helper simply streams each Markdown file from the preprocessing directory straight to the backend."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
